{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b41daa5",
   "metadata": {},
   "source": [
    "**1: For which of the models already studied is the curse of dimensionality relevant and explain why**\n",
    "- `k` ближайших соседей, k-means, DBSCAN и другие метрические алгоритмы: в высоких размерностях расстояния между точками становятся почти одинаковыми, поэтому пропадает локальность, требуется экспоненциально больше данных.\n",
    "- Перебор по сетке (grid search): размер сетки растёт как `O(b^d)`, что делает вычисления и хранение данных невозможными без снижения размерности.\n",
    "- Любые алгоритмы, опирающиеся на локальные окрестности в исходном признаковом пространстве: пространство становится разреженным, поэтому локальные предположения (гладкость, близость соседей) больше не выполняются.\n",
    "\n",
    "**2: What is the difference between PCA and SVD?**\n",
    "- PCA (метод главных компонент) — статистическая процедура: центрирует данные, ищет ортонормированные направления максимальной дисперсии, интерпретирует их как главные компоненты и отбрасывает малую дисперсию.\n",
    "- SVD (сингулярное разложение) — линейно-алгебраический инструмент: раскладывает любую матрицу `X = UΣVᵀ`, используется как вычислительный шаг внутри PCA, но не требует центрирования и сам по себе не выбирает число компонент.\n",
    "- На практике PCA вычисляют через SVD для центрированной матрицы признаков; сингулярные векторы `V` дают направления PCA, а квадраты сингулярных значений `σ²` пропорциональны дисперсиям компонентов.\n",
    "\n",
    "**3: What is the difference between NMF and SVD?**\n",
    "- SVD разлагает любую матрицу `X` на `UΣVᵀ` с ортогональными `U`, `V` и допускает отрицательные значения в компонентах; оптимальность глобальная, решение единственно с точностью до знака.\n",
    "- NMF ищет факторизацию `X ≈ WH` с ограничением неотрицательности элементов `W` и `H`, что обеспечивает аддитивную, интерпретируемую композицию признаков, но решение не уникально и обычно ищется итеративно.\n",
    "- NMF больше подходит для задач, где данные и факторы имеют естественную неотрицательность (тексты, спектры, изображения), тогда как SVD удобен для оптимального ранжирования и сжатия без интерпретационных ограничений.\n",
    "\n",
    "**4: Describe the structure of the Locally Linear Embedding dimension reduction algorithm.**\n",
    "1. Граф соседства: для каждой точки нужно найти `k` ближайших соседей в исходном пространстве и построить разреженный граф, фиксирующий локальную геометрию.\n",
    "2. Реконструкционные веса: для каждой точки нужно решить задачу наименьших квадратов с ограничением аффинности, выражая точку через линейную комбинацию её соседей; веса сохраняют локальную линейную структуру.\n",
    "3. Низкоразмерное вложение: нужно найти координаты `Y`, минимизирующие несоответствие между исходными весами и реконструкцией в новом пространстве; оптимальное `Y` получается из нескольких младших векторов разреженной матрицы стоимости.\n",
    "4. Выбор размерности: нужно оставить `d` наименьших ненулевых собственных векторов (после нулевого, отвечающего за сдвиги), чтобы получить вложение нужной размерности."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
